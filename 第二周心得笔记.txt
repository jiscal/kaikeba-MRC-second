心得笔记
   上周主要讲了常见的机器阅读理解模型的相关知识。1 从词的表示，从最初的词袋的表示one-hot开始
由于one-hot的缺点没有讲究词顺序，引入了word2vec中两种skip-gram和CBOW两种方法表示词向量。
它的表现词的方式已经进行了极大提升，由于上面的两种方法中采用一个框框边界，一种局部信息
所以它并没有考虑全文语言特性。所以在此基础上有了Glove，它有了一个全局的先验的统计信息，在
此基础上在统计共现信息，也就是从基于词的共现矩阵和Glove模型学习词向量。目前大多应用的时候
我们可以不用直接训练，从官网上下来它的预训练好的词向量，比较方便。2讲到卷积神经网络，它常
用来处理图像和时间序列数据。它与全连接不同在于它的计算量没有全连接多，参数少，如果参数过多
势必会造成过拟合现象。在进行全连接时候需要将多维数据全铺，势必也会造成局部的关联信息的缺失。
讲到卷积神经网络，卷积操作是一种数学计算，我的理解为可以用一个n*m的卷积核在图像区域上面
进行滑到，做了一个加权求和的操作。从而达到特征提取。一般经过卷积之后会采用池化，池化也会有
池化核的作用。相当于一种降维作用。我们做文本时候，可以采用一维的卷积操作，维度就词向量的
维度。文本的多通道可以理解为采用不同方式获得的多个向量的表达方式。一维卷积安照词的移动顺序
上进行滑动，提取特征，然后在经过最大池化操作降维，将降维的多通过数据进行融合也就形成以一通道
的词向量表示了，后续可以全连接+softmax操作来达到文本文类的效果。3、attention的理解 权重分配的
思想。也就是在一系列词中，有些词对于我们来说很重要，我们就赋予他很高的权重，具体的一般就是权重
乘以对应词的hidden state，它考虑所有的隐藏层信息，与早先的encoder-decoder模型不同，它只需要获取
encoder中最后一个hidden state，当序列比较长的时候，它会丢失信息，性能变得很差。权重的计算从词
向量中随机初始化三个矩阵WQ,WK,WV获得Q,K,V。将Q,V计算相似度（相似度计算方式多种），得到权值，
权值归一化然后个V进行加权求和。4attention早期的应用促进了阅读理解的进步，开创了一维匹配模型和二
维匹配模型。它们都适用完形填空的任务。不同的是一维匹配模型，它会将问题编码作为固定长度向量，
计算文档 D 每个词在特定问题上下文向量中作为答案的概率。二维则是将问题每一个词编码，形成问题中词与
文档中词的二维匹配结构。5谈论了上述的诸多知识，是为了引入了双向注意力机制，Bi-DAF。它的整个流程我
的理解是，首先经过字符的词嵌入，单词的词嵌入编码经过一个高速网络通道，再作为一个LSTM层的输入
(以下的LSTM层都是双向的)分别获得context的词向量H和问题的词向量U(它们都是二维向量，包括词向量和字符向量),
他们经过交互层，交互层的作用是分别计算文章对问题的注意力和问题对文章的注意力。他们的不同在于，
首先都是通过H,u获取相似矩阵Sij，文章对问题的注意力是将Sij进行softmax操作然后将其与U中向量进行相乘求和。
问题对文章的注意力则是将sij中获得每一行最大值，然后与H中向量进行相乘求和,由于此时维数变成n*1的矩，
所以需要将其重复m次。将上述获得的注意力进行拼接（H,H的注意力，H和H的注意力点积，H和U的注意力点积）获得
一个8维的G向量,再将它经过LSTM获得二维M向量。这就到了输出层，通过G,M,M的平方(M通过LSTM获得)分别获得开始
位置的概率和结束位置概率，计算交叉熵损失。再通过反向传播，调整模型参数，训练，从而达到很好的效果。
   关于作业。我的思路是看代码整体架构，这样一方面可以帮助我更好理解整个模型的算法思想。涉及到具体时，
将代码拆分成若干单元，了解每个函数的作用，这方面可以通过调试来掌握。所以我理解到原先代码是将句子直接
进行字符的向量操作，未涉及到单词，而现在老师的要求应该是将词与字符的结合向量来做。由于本人深度架构代码
方面有些欠缺，所以这方面凭借一边查阅一边理解写下的，这方面后期需要提高的。以上就是我的上周的学习心得。

